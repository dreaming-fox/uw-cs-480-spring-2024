{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project: Predicting 6 Vital Plant Traits\n",
    "\n",
    "Stephen Hwang (#20889701)\\\n",
    "CS 480 Spring 2024\\\n",
    "Due August 12, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import timm\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Settable Fields for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "EARLY_STOP_R2 = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plant Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlantDataset(Dataset):\n",
    "    def __init__(self, img_dir, csv_file, norm_file, img_transform, train=True):\n",
    "        # images\n",
    "        self.img_dir = img_dir\n",
    "        self.img_transform = img_transform\n",
    "        self.img_names = [img_name for img_name in os.listdir(self.img_dir)]\n",
    "        self.img_paths = [os.path.join(self.img_dir, img_name) for img_name in self.img_names]\n",
    "\n",
    "        # additional attributes\n",
    "        self.trait_data = pd.read_csv(csv_file)\n",
    "        self.trait_data = self.trait_data.sort_values(by=self.trait_data.columns[0], key=lambda x: x.astype(str), ascending=True)\n",
    "        self.ids = np.array(self.trait_data.iloc[:, 0].values)\n",
    "        self.attrs = np.array(self.trait_data.iloc[:, 1:164].values)\n",
    "        self.targets = np.array(self.trait_data.iloc[:, 164:].values)\n",
    "        if not train:\n",
    "            self.targets = [[0, 0, 0, 0, 0, 0] for _ in self.targets]\n",
    "\n",
    "        # information for attribute normalization\n",
    "        self.max_min_data = pd.read_csv(norm_file)\n",
    "        self.attr_max = self.max_min_data.iloc[0, 0:163].values\n",
    "        self.attr_min = self.max_min_data.iloc[1, 0:163].values\n",
    "        self.target_max = self.max_min_data.iloc[0, 163:].values\n",
    "        self.target_min = self.max_min_data.iloc[1, 163:].values\n",
    "\n",
    "        # target manipulation\n",
    "        if train:\n",
    "            # find data points with target outliers\n",
    "            target_means = np.mean(self.targets, axis=0)\n",
    "            target_std_devs = np.std(self.targets, axis=0)\n",
    "            target_lower = target_means - (3 * target_std_devs)\n",
    "            target_upper = target_means + (3 * target_std_devs)\n",
    "            target_outliers = []\n",
    "\n",
    "            # remove data points with target outliers\n",
    "            for i, val in enumerate(self.targets):\n",
    "                if any(val < target_lower) or any(val > target_upper):\n",
    "                    target_outliers.append(i)\n",
    "            self.img_names = [ele for idx, ele in enumerate(self.img_names) if idx not in target_outliers]\n",
    "            self.img_paths = [ele for idx, ele in enumerate(self.img_paths) if idx not in target_outliers]\n",
    "            self.ids = np.array([ele for idx, ele in enumerate(self.ids) if idx not in target_outliers])\n",
    "            self.attrs = np.array([ele for idx, ele in enumerate(self.attrs) if idx not in target_outliers])\n",
    "            self.targets = np.array([ele for idx, ele in enumerate(self.targets) if idx not in target_outliers])\n",
    "\n",
    "        # asserts for correlation between image and attribute data\n",
    "        assert len(self.img_names) == len(self.img_paths)\n",
    "        assert len(self.img_paths) == len(self.ids)\n",
    "        assert len(self.ids) == len(self.attrs)\n",
    "        assert len(self.attrs) == len(self.targets)\n",
    "        for i in range(0, len(self.img_names)):\n",
    "            id, _ = os.path.splitext(self.img_names[i])\n",
    "            assert str(self.ids[i]) == id\n",
    "\n",
    "        # asserts for correlation between attribute and normalization data\n",
    "        assert len(self.attr_max) == len(self.attrs[0])\n",
    "        assert len(self.attr_min) == len(self.attrs[0])\n",
    "        assert len(self.target_max) == len(self.targets[0])\n",
    "        assert len(self.target_min) == len(self.targets[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # load item\n",
    "        image = self.img_transform(Image.open(self.img_paths[idx]))\n",
    "        attrs = torch.tensor(self.attrs[idx], dtype=torch.float32)\n",
    "        targets = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        id = self.ids[idx]\n",
    "\n",
    "        # normalize attributes\n",
    "        min_attr_values = torch.tensor(self.attr_min, dtype=torch.float32)\n",
    "        max_attr_values = torch.tensor(self.attr_max, dtype=torch.float32)\n",
    "        range_attr_values = max_attr_values - min_attr_values\n",
    "        range_attr_values = torch.where(range_attr_values == 0, torch.tensor(1.0), range_attr_values)\n",
    "        norm_attrs = (attrs - min_attr_values) / range_attr_values\n",
    "        \n",
    "        # normalize targets\n",
    "        min_target_value = torch.tensor(self.target_min, dtype=torch.float32)\n",
    "        max_target_value = torch.tensor(self.target_max, dtype=torch.float32)\n",
    "        range_target_value = max_target_value - min_target_value\n",
    "        range_target_value = torch.where(range_target_value == 0, torch.tensor(1.0), range_target_value)\n",
    "        norm_targets = (targets - min_target_value) / range_target_value\n",
    "\n",
    "        return image, norm_attrs, norm_targets, id\n",
    "    \n",
    "    def get_image_mean_std(self):\n",
    "        rgb_values = np.concatenate(\n",
    "            [Image.open(img).getdata() for img in self.img_paths], \n",
    "            axis=0\n",
    "        ) / 255.\n",
    "        mean = np.mean(rgb_values, axis=0)\n",
    "        std = np.std(rgb_values, axis=0)\n",
    "        return mean, std\n",
    "    \n",
    "    def reverse_normalization(self, normalized_val):\n",
    "        range_target_value = self.target_max - self.target_min\n",
    "        val = (normalized_val * ([rng if rng != 0 else 1 for rng in range_target_value])) + self.target_min\n",
    "        return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.1),\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(1, 1.2)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.44636917, 0.45045858, 0.33603736], [0.21836502, 0.20886066, 0.21879451])\n",
    "])\n",
    "\n",
    "train_dataset = PlantDataset(img_dir='./data/train_images', csv_file='./data/train.csv', norm_file='./data/max_mins.csv', img_transform=transform)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [train_dataset.__len__() - 4000, 4000])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, in_features)\n",
    "        self.bn2 = nn.BatchNorm1d(in_features)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = models.resnet101(weights='DEFAULT')\n",
    "        # self.cnn = models.mobilenet_v2(weights='DEFAULT')\n",
    "        # self.cnn = timm.create_model('inception_resnet_v2', pretrained=True)\n",
    "        self.final_fc = nn.Sequential(\n",
    "            nn.Linear(1000, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.cnn(img)\n",
    "        x = self.final_fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribute NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributeNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block1 = ResidualBlock(163, 256)\n",
    "        self.block2 = ResidualBlock(163, 256)\n",
    "        self.block3 = ResidualBlock(163, 128)\n",
    "        self.fc_out = nn.Linear(163, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.image_cnn = ImageCNN()\n",
    "        self.attr_nn = AttributeNN()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 + 128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, img, attrs):\n",
    "        img_features = self.image_cnn(img)\n",
    "        attr_features = self.attr_nn(attrs)\n",
    "        combined = torch.cat((img_features, attr_features), dim=1)\n",
    "        return self.fc(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CombinedNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0005, weight_decay=0.00001)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training-Validation Loop (i.e. Train New Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    train_inputs = 0\n",
    "    print('-')\n",
    "\n",
    "    # iterate through training batch\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        images, attrs, targets = data[0].float().to(device), data[1].float().to(device), data[2].float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(images, attrs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * targets.size(0)\n",
    "        train_inputs += targets.size(0)\n",
    "\n",
    "    writer.add_scalar(\"Average Training Loss\", train_loss / train_inputs, epoch)\n",
    "    print(f'Epoch {epoch} complete, average loss: {train_loss / train_inputs}')\n",
    "    \n",
    "    # compute validation loss and R2 score\n",
    "    net.eval()\n",
    "    target_preds = []\n",
    "    target_true = []\n",
    "    val_loss = 0\n",
    "    val_inputs = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader, 0):\n",
    "            images, attrs, targets = data[0].float().to(device), data[1].float().to(device), data[2].float().to(device)\n",
    "            \n",
    "            outputs = net(images, attrs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            val_loss += loss.item() * targets.size(0)\n",
    "            val_inputs += targets.size(0)\n",
    "            target_preds.append(outputs.cpu().numpy())\n",
    "            target_true.append(targets.cpu().numpy())\n",
    "\n",
    "    target_preds = np.concatenate(target_preds)\n",
    "    target_true = np.concatenate(target_true)\n",
    "\n",
    "    r2 = r2_score(target_true, target_preds, multioutput='raw_values')\n",
    "    print(f'Epoch {epoch} complete, average val loss: {val_loss / val_inputs}')\n",
    "    print(f'R2 score for val set epoch {epoch}: {r2}')\n",
    "\n",
    "    writer.add_scalar(\"Average Validation Loss\", val_loss / val_inputs, epoch)\n",
    "    writer.add_scalar(\"Validation R2 Score\", np.mean(r2), epoch)\n",
    "\n",
    "    if any(r2 > [EARLY_STOP_R2 for _ in range(0, 6)]):\n",
    "        break\n",
    "\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "print('-')\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Model and Generate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PlantDataset(img_dir='./data/test_images', csv_file='./data/test.csv', norm_file='./data/max_mins.csv', img_transform=transform, train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_ids = []\n",
    "test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        images, attrs, ids = data[0].float().to(device), data[1].float().to(device), data[3]\n",
    "        outputs = net(images, attrs)\n",
    "        test_preds.extend(outputs.cpu().numpy())\n",
    "        test_ids.extend(ids.numpy())\n",
    "\n",
    "normal_test_preds = [train_dataset.reverse_normalization(pred) for pred in test_preds]\n",
    "\n",
    "filename = f'./outputs/test.csv'\n",
    "with open(filename, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    header = ['id', 'X4', 'X11', 'X18', 'X26', 'X50', 'X3112'] \n",
    "    writer.writerow(header)\n",
    "    for id, predictions in zip(test_ids, normal_test_preds):\n",
    "        row = [id] + list(predictions)\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
